Meant to be transformer version of [gan-hacks](/gan/gan-hacks)

## Good resources
- [Converting a From-Scratch GPT Architecture to Llama 2](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-gpt-to-llama2.ipynb?trk=public_post_comment-text)
- [LLAMA source code](https://github.com/meta-llama/llama/blob/main/llama/model.py)
- [Transformers and Multi-Head Attention](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)

## Embeddings
- [LLM2Vec](https://arxiv.org/pdf/2404.05961)
  - Remove the mask from the attention training
  - Use [MASK] tokens
  - Contrastive learning
- 

## Post training
- [Speculative sampling](https://jaykmody.com/blog/speculative-sampling/)

## Interference
- [Mastering LLM Techniques: Inference Optimization](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/)
- [KV cache](https://medium.com/@joaolages/kv-caching-explained-276520203249)

## TODO
- Benchmark [DeepSeek v3](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/model.py) layers also
- 
