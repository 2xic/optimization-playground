# 2 consider

## Sources
- [https://spinningup.openai.com/en/latest/spinningup/keypapers.html](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)
- [https://lilianweng.github.io/](https://lilianweng.github.io/)


# Some I'm considering
### Computer vision
[What You Get Is What You See: A Visual Markup Decompiler](https://arxiv.org/pdf/1609.04938v1.pdf)

### RL 
[Transformers are Sample Efficient World Models](https://arxiv.org/pdf/2209.00588.pdf)

## NLP 
[Large Language Models Can Self-Improve](https://arxiv.org/pdf/2210.11610.pdf)
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

## GAN
[Multi-Adversarial Variational Autoencoder Networks](https://arxiv.org/pdf/1906.06430.pdf)

## NN architecture
[Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/pdf/1608.05343.pdf)
[ZerO Initialization: Initializing Neural Networks with only Zeros and Ones](https://openreview.net/pdf?id=1AxQpKmiTc)
[Dynamic Routing Between Capsules](https://arxiv.org/pdf/1710.09829.pdf)
[The Forward-Forward Algorithm: Some Preliminary Investigations](https://www.cs.toronto.edu/~hinton/FFA13.pdf)

## Recommendations
[Monolith: Real Time Recommendation System With Collisionless Embedding Table](https://arxiv.org/pdf/2209.07663.pdf)
