# 2 consider

## Source for papers 2 consider
- [https://paperswithcode.com/](https://paperswithcode.com/)
- [https://spinningup.openai.com/en/latest/spinningup/keypapers.html](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)
- [https://lilianweng.github.io/](https://lilianweng.github.io/)
- [https://www.vicarious.com/publications/](https://www.vicarious.com/publications/)
- [https://openai.com/publications/](https://openai.com/publications/)
- [https://distill.pub/](https://distill.pub/)
- [https://github.com/lucidrains](https://github.com/lucidrains)
- 
# Some I'm considering

## Self supervised 
:)

### Computer vision
[https://vcrs.wpengine.com/wp-content/uploads/2020/03/1611.02788.pdf](https://vcrs.wpengine.com/wp-content/uploads/2020/03/1611.02788.pdf)

### RL 
[Transformers are Sample Efficient World Models](https://arxiv.org/pdf/2209.00588.pdf)
[Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation](https://arxiv.org/pdf/2106.04399.pdf)
[Solving rubicks cube with robotic hand](https://arxiv.org/pdf/1910.07113.pdf)

## NLP 
[Large Language Models Can Self-Improve](https://arxiv.org/pdf/2210.11610.pdf)
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
[Theory of Mind May Have Spontaneously Emerged in Large Language Models](https://arxiv.org/pdf/2302.02083.pdf)
[Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/pdf/2112.11446.pdf)
[Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/pdf/2201.11990.pdf)

## GAN
[Neural Networks with Recurrent Generative Feedback](https://arxiv.org/pdf/2007.09200.pdf)
[Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/pdf/1905.02175.pdf)

## NN architecture
[https://arxiv.org/pdf/2012.14905.pdf](Meta Learning Backpropagation And Improving It)
[https://arxiv.org/pdf/2212.05153.pdf](Algorithmic progress in computer vision)

## Recommendations
:)
