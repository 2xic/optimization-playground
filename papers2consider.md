# 2 consider

## Sources
- [https://spinningup.openai.com/en/latest/spinningup/keypapers.html](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)
- [https://lilianweng.github.io/](https://lilianweng.github.io/)
- [https://www.vicarious.com/publications/](https://www.vicarious.com/publications/)
- [https://openai.com/publications/](https://openai.com/publications/)

# Some I'm considering

## Self supervised 
[Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language](https://arxiv.org/pdf/2212.07525.pdf)
    - [Data2vec 2.0: Highly efficient self-supervised learning for vision, speech and text ](https://ai.facebook.com/blog/ai-self-supervised-learning-data2vec/?utm_source=pocket_reader)

### Computer vision
[What You Get Is What You See: A Visual Markup Decompiler](https://arxiv.org/pdf/1609.04938v1.pdf)
[https://vcrs.wpengine.com/wp-content/uploads/2020/03/1611.02788.pdf](https://vcrs.wpengine.com/wp-content/uploads/2020/03/1611.02788.pdf)

### RL 
[Transformers are Sample Efficient World Models](https://arxiv.org/pdf/2209.00588.pdf)
[https://arxiv.org/pdf/2106.04399.pdf](https://arxiv.org/pdf/2106.04399.pdf)

## NLP 
[Large Language Models Can Self-Improve](https://arxiv.org/pdf/2210.11610.pdf)
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

## GAN
[Multi-Adversarial Variational Autoencoder Networks](https://arxiv.org/pdf/1906.06430.pdf)

## NN architecture
[https://arxiv.org/pdf/2012.14905.pdf](Meta Learning Backpropagation And Improving It)

## Recommendations
:)
