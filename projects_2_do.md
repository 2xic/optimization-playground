Sometimes I forget the projects had planned, better to write it down here.

## Ideas
- (WIP) Implement, and evaluate some embeddings `nlp/embeddings-experiments`

- Implement the [forward-forward algorithm](https://arxiv.org/abs/2212.13345) 

- Based on reading [Why do tree-based models still outperform deep learning on tabular data?](https://arxiv.org/pdf/2207.08815.pdf) they mentioned NNs are more affected by unimportant features. Create a small project to see if this is true. For instance by just padding some noise to an image vector maybe

- Try to see if you can replicate something like [Deep double descent](https://openai.com/research/deep-double-descent). This is not 2 far away from something like `playground/epiphany`

- Train an RL agent on curiosity reward. There is already some notes in `reinforcement-learning/exploration-by-random-network-distillation` 

- [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/pdf/1905.02175.pdf) has some cool plots, can you reproduce some of them ? 

- [RL^2 - fast reinforcement learning via slow reinforcement learning](https://arxiv.org/pdf/1611.02779.pdf) -> Implement the core idea