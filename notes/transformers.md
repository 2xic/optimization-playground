### [LONGNET: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/pdf/2307.02486.pdf)
Use dilated attention to be scale up.
- Drop in replacement fro existing attention layer
- Much less compute intensive (Figure 5)

[tweet](https://twitter.com/giffmana/status/1676864336764055552?s=12)

### [TRANSFORMER-BASED WORLD MODELS ARE HAPPY WITH 100K INTERACTIONS](https://arxiv.org/pdf/2303.07109.pdf)
- TODO ? 

### [GLU Variants Improve Transformer](https://arxiv.org/pdf/2002.05202v1.pdf)
- TODO ? 
