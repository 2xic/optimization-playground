## [Transcending Scaling Laws with 0.1% Extra Compute](https://arxiv.org/pdf/2210.11399)
- Figure 1 shows quite the difference 
  - There is a 4.4 million TPUv4 hours difference between when U-PaLM reaches the same accuracy as PaLM on the NLP tasks they tested on
- Training
  - Same data aas for PaLM
  - PrefixLM architecture for U-PaLM
- Where does the 0.1% extra compute come from ? 
  - The UL2 training objective mixture
  - UL2R-(estore) is the name of the method


