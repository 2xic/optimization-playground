## [Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators)
- "They use a transformer that operates on space time patches of video and image latent codes"
  - The patches are generated by a encoder network
  - They actually use a diffusion transformer
  - They also observe here that the models capabilities scales with compute  
- The model is trained conditionally on text input against various videos
- The results looks quite promising
  - The model can also sample at various aspect ratios etc. all from the same model

[Overview](https://openai.com/sora), it's only available to red teams atm. There seems to have been done some independent [evaluation](https://arxiv.org/pdf/2402.17403.pdf) and it beats for instance [Pika](https://pika.art/home)


## [Stable Video Diffusion](https://static1.squarespace.com/static/6213c340453c3f502425776e/t/655ce779b9d47d342a93c890/1700587395994/stable_video_diffusion.pdf)
I knew Stability also had launched a model for video generation, but they don't seem to use the transformer
- They seem to use some of the existing model ideas used for 2D latent image diffusion models and insert temporal layers into them  
- They seem to need to do a lot more pretraining and finetuning than what Sora had to, but hard to tell without seeing all the details of Sora

The authors also had this [talk](https://www.youtube.com/watch?v=U3J6R9gfUhU) at the AGI house.

## [SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion](https://arxiv.org/pdf/2403.12008)
[Blog post](https://stability.ai/news/introducing-stable-video-3d)

- Make one 2d image into 3d view
- Figure 2 shows the overview of the architecture. Quite nice.
